<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>【ng公开课笔记08】SVM | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="1.优化目标首先先来复习一下逻辑回归：下面是逻辑回归的假设函数       如果有一个y=1的样本，如果能够让它能够被正确分类，即希望 $h_\theta(x)=1$,就需要 $\theta^Tx&amp;gt;&amp;gt;0$ 。y=0时同理      下面是逻辑回归一个样本的代价函数计算公式，总代价函数即是每个样本的代价求和再取均值               通过函数图像可以看到，当$z=\theta^">
<meta name="keywords" content="SVM">
<meta property="og:type" content="article">
<meta property="og:title" content="【ng公开课笔记08】SVM">
<meta property="og:url" content="https://kingsea0-0.github.io/2017/07/24/2017-7-23-【ng公开课笔记08】SVM/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="1.优化目标首先先来复习一下逻辑回归：下面是逻辑回归的假设函数       如果有一个y=1的样本，如果能够让它能够被正确分类，即希望 $h_\theta(x)=1$,就需要 $\theta^Tx&amp;gt;&amp;gt;0$ 。y=0时同理      下面是逻辑回归一个样本的代价函数计算公式，总代价函数即是每个样本的代价求和再取均值               通过函数图像可以看到，当$z=\theta^">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/js245wqiq4okcoz930sj1mb6/image_1blk0li1a5pb1u87duf1mblvcv9.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/krv45leod1i18vm2inzh2p5s/image_1blk0v4sc2i9v1h1hf21ta61fk3m.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/ouhwxnv0xldp7hyhsozd87ov/image_1blk115gp12t9122u1e9t1ch71c1613.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/repbwvedv8rye06wrw4ia8wt/image_1blk15q3r1husdg91p6p491ruj1t.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/wos3zs3baf2xoybxzlk1m90u/image_1blk17dlvp8i13gfg1vgkkqtj2a.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/7ud0bmeg0jola2spy26dgstd/image_1blk1euln1stm1hqf1hrbrps1c8i2n.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/vpenjwwir7vrnkgk2cbqbey9/image_1blk3hioemch18jq12ru146mb6234.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/t5g9vlb2225qb0qprlmwidrj/image_1blk3qad616341tk4t8hvcd1ppo3u.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/yrswq248db2biuv8xsjb19hs/image_1blk414qj1futdn8f3p88p7r14b.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/6vjeowsjcu9uif3cit7vd1si/image_1blk44qa095etfk1dqtm381pcf55.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/oy8bn3yuocps9pgnajmmeosz/image_1blk44f111kl3ntr1h5q1h5f1mdv4o.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/7mattcop5fgb8yco9gth66yz/image_1blk4qf8ndc71rln1se870a2mh5i.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/48ewu5i8qusihy2be9uy76a9/image_1blk4soi1fui80l164klmp1k9n5v.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/a461lksh9gybhpif1p5gtx34/image_1blk6g7mo5r1akaifd1g601ba36c.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/sb8p3bdry35qkz6mtlu3l8j7/image_1blk6ldleker4degq91p1qhes6p.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/5ubi16a3esvvkcfjgixsmdx1/image_1blk76i1d1sukbm01mcli71m7o76.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/j0yaeif6ou2snen01kvjhjal/image_1blk77e9898b3nhmtu1ahd1j807j.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/x7cfof2pbf5n07j2y6lcwhlk/image_1blkbmp3g10rl1t75gnu1043op580.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/ckkq7f9pnij7wtddak33yh96/image_1blkc9h2qbn51t259nqnl0srp8d.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/pljyln4y08yg8bmpjdxgaw6j/image_1blkcbcd31iqcpcvn20frlmt78q.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/tpa7po7cj5j9aqgeyxijpwtz/image_1blkcth651fjv41cbqhvq1107i97.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/kb3bcb70n41v99qmmnqzqzot/image_1blkdfo7q7cs13dq43v1tsf1h389k.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/l3m8weskz5zqjcsx4xd9evgs/image_1blkdnt7g1o081j2ht6t8sq122ja1.png">
<meta property="og:updated_time" content="2018-11-28T08:16:31.053Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="【ng公开课笔记08】SVM">
<meta name="twitter:description" content="1.优化目标首先先来复习一下逻辑回归：下面是逻辑回归的假设函数       如果有一个y=1的样本，如果能够让它能够被正确分类，即希望 $h_\theta(x)=1$,就需要 $\theta^Tx&amp;gt;&amp;gt;0$ 。y=0时同理      下面是逻辑回归一个样本的代价函数计算公式，总代价函数即是每个样本的代价求和再取均值               通过函数图像可以看到，当$z=\theta^">
<meta name="twitter:image" content="http://static.zybuluo.com/yhsdba/js245wqiq4okcoz930sj1mb6/image_1blk0li1a5pb1u87duf1mblvcv9.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kingsea0-0.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2017-7-23-【ng公开课笔记08】SVM" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/07/24/2017-7-23-【ng公开课笔记08】SVM/" class="article-date">
  <time datetime="2017-07-24T02:00:00.000Z" itemprop="datePublished">2017-07-24</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      【ng公开课笔记08】SVM
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="1-优化目标"><a href="#1-优化目标" class="headerlink" title="1.优化目标"></a>1.优化目标</h1><p>首先先来复习一下逻辑回归：<br>下面是逻辑回归的假设函数   </p>
<p><img src="http://static.zybuluo.com/yhsdba/js245wqiq4okcoz930sj1mb6/image_1blk0li1a5pb1u87duf1mblvcv9.png" alt="image_1blk0li1a5pb1u87duf1mblvcv9.png-160.5kB"></p>
<p>  如果有一个y=1的样本，如果能够让它能够被正确分类，即希望 $h_\theta(x)=1$,就需要 $\theta^Tx&gt;&gt;0$ 。y=0时同理   </p>
<p>  下面是逻辑回归一个样本的代价函数计算公式，总代价函数即是每个样本的代价求和再取均值<br>  <img src="http://static.zybuluo.com/yhsdba/krv45leod1i18vm2inzh2p5s/image_1blk0v4sc2i9v1h1hf21ta61fk3m.png" alt="image_1blk0v4sc2i9v1h1hf21ta61fk3m.png-90.5kB">       </p>
<p>  <img src="http://static.zybuluo.com/yhsdba/ouhwxnv0xldp7hyhsozd87ov/image_1blk115gp12t9122u1e9t1ch71c1613.png" alt="image_1blk115gp12t9122u1e9t1ch71c1613.png-58.1kB"></p>
<p>  通过函数图像可以看到，当$z=\theta^Tx&gt;&gt;0$时，cost-&gt;0,与上述结论相似。 </p>
<p>  在svm中我们在上述代价函数基础上进行简化：  </p>
<p><img src="http://static.zybuluo.com/yhsdba/repbwvedv8rye06wrw4ia8wt/image_1blk15q3r1husdg91p6p491ruj1t.png" alt="image_1blk15q3r1husdg91p6p491ruj1t.png-143.2kB">  </p>
<p>得到SVM的代价函数：  </p>
<p><img src="http://static.zybuluo.com/yhsdba/wos3zs3baf2xoybxzlk1m90u/image_1blk17dlvp8i13gfg1vgkkqtj2a.png" alt="image_1blk17dlvp8i13gfg1vgkkqtj2a.png-389.6kB">  </p>
<p>与逻辑回归区别，除了代价函数的变化，系数也进行了一些调整。  </p>
<h1 id="2-大边界的直观理解"><a href="#2-大边界的直观理解" class="headerlink" title="2.大边界的直观理解"></a>2.大边界的直观理解</h1><p><img src="http://static.zybuluo.com/yhsdba/7ud0bmeg0jola2spy26dgstd/image_1blk1euln1stm1hqf1hrbrps1c8i2n.png" alt="image_1blk1euln1stm1hqf1hrbrps1c8i2n.png-232.3kB">  </p>
<p>这是上一部分我们得出的svm代价函数模型。在逻辑回归中，我们只需要z&gt;0就可以将样本分类为1，z&lt;0将样本分类为0。但在SVM中，只有z&gt;1或者z&lt;-1时才能将样本确定为类型1或0。相当于在SVM中加入了一个安全（距离）因子。  </p>
<p>那这个因子导致了什么结果？从一个特特例来看，将C设为一个很大的数如100000，观察支持向量机  </p>
<p><img src="http://static.zybuluo.com/yhsdba/vpenjwwir7vrnkgk2cbqbey9/image_1blk3hioemch18jq12ru146mb6234.png" alt="image_1blk3hioemch18jq12ru146mb6234.png-95.4kB">   </p>
<p>如果C很大，我们想要最小化代价函数，就希望得到一个第一项为0的最优解。当我们最优化$\theta$函数后，会得到一个决策边界  </p>
<p><img src="http://static.zybuluo.com/yhsdba/t5g9vlb2225qb0qprlmwidrj/image_1blk3qad616341tk4t8hvcd1ppo3u.png" alt="image_1blk3qad616341tk4t8hvcd1ppo3u.png-60.6kB">   </p>
<p>支持向量机会选择黑色的边界。但其他的边界也可以分开样本（但看起来不自然），那为什么会选择黑色的？  </p>
<p><img src="http://static.zybuluo.com/yhsdba/yrswq248db2biuv8xsjb19hs/image_1blk414qj1futdn8f3p88p7r14b.png" alt="image_1blk414qj1futdn8f3p88p7r14b.png-82.5kB"></p>
<p>如果我们在样本边界做分界线的平行线，我们可以看到黑色分界线和样本间有更大的最短距离，所以支持向量机也被称为大间距分类器。    </p>
<p>我们来看一下C这个系数的作用：</p>
<p>像下面的样本，我们可以得到一条垂直于x轴的分界线</p>
<p><img src="http://static.zybuluo.com/yhsdba/6vjeowsjcu9uif3cit7vd1si/image_1blk44qa095etfk1dqtm381pcf55.png" alt="image_1blk44qa095etfk1dqtm381pcf55.png-40.5kB">  </p>
<p>但如果加入了一个异常点</p>
<p><img src="http://static.zybuluo.com/yhsdba/oy8bn3yuocps9pgnajmmeosz/image_1blk44f111kl3ntr1h5q1h5f1mdv4o.png" alt="image_1blk44f111kl3ntr1h5q1h5f1mdv4o.png-55.1kB">  </p>
<p>当C很大时，我们将会得到粉色的分界线，而C较小时，依然得到黑色的分界线。<br>（我们可以将C看作$1/\lambda$,当C很大，即正则化系数很小，容易过拟合）</p>
<h1 id="3-大边界分类器的数学原理"><a href="#3-大边界分类器的数学原理" class="headerlink" title="3. 大边界分类器的数学原理"></a>3. 大边界分类器的数学原理</h1><p>svm的目标函数（第一项优化为0）：  </p>
<p><img src="http://static.zybuluo.com/yhsdba/7mattcop5fgb8yco9gth66yz/image_1blk4qf8ndc71rln1se870a2mh5i.png" alt="image_1blk4qf8ndc71rln1se870a2mh5i.png-17.6kB"></p>
<p>为了方便，我们将$\theta_0=0$，将特征数n=2</p>
<p>目标函数可以写为<br>$$1/2(\theta_1^2+\theta_2^2)=1/2(\sqrt{\theta_1^2+\theta_2^2}=1/2||\theta||^2)$$<br>将x也画在图像上</p>
<p><img src="http://static.zybuluo.com/yhsdba/48ewu5i8qusihy2be9uy76a9/image_1blk4soi1fui80l164klmp1k9n5v.png" alt="image_1blk4soi1fui80l164klmp1k9n5v.png-205.5kB">  </p>
<p>$$\theta^T<em>x=p</em>\mid\theta\mid$$ </p>
<p><img src="http://static.zybuluo.com/yhsdba/a461lksh9gybhpif1p5gtx34/image_1blk6g7mo5r1akaifd1g601ba36c.png" alt="image_1blk6g7mo5r1akaifd1g601ba36c.png-131.3kB">    </p>
<p>$\theta$ 是和决策边界正交的，我们可以看到，如果是正样本，p&gt;0,负样本则p&lt;0如果选择如图的边界p会非常小.而我们优化目标函数的时候，我们需要$p\midθ\mid&gt;=1$,但在这里p很小，就需要θ很大。但我们的优化目标是找到一个较小的theta。所以我们需要选择一个边界使得p最大。</p>
<p><img src="http://static.zybuluo.com/yhsdba/sb8p3bdry35qkz6mtlu3l8j7/image_1blk6ldleker4degq91p1qhes6p.png" alt="image_1blk6ldleker4degq91p1qhes6p.png-328.9kB">  </p>
<h1 id="4-核函数"><a href="#4-核函数" class="headerlink" title="4.核函数"></a>4.核函数</h1><p>对于之前讨论过的非线性分类问题，我们之前通过多项式的形式解决。但通常代价较大，所以我们换一种方式来解决。</p>
<p><img src="http://static.zybuluo.com/yhsdba/5ubi16a3esvvkcfjgixsmdx1/image_1blk76i1d1sukbm01mcli71m7o76.png" alt="image_1blk76i1d1sukbm01mcli71m7o76.png-120.4kB">  </p>
<p><img src="http://static.zybuluo.com/yhsdba/j0yaeif6ou2snen01kvjhjal/image_1blk77e9898b3nhmtu1ahd1j807j.png" alt="image_1blk77e9898b3nhmtu1ahd1j807j.png-107.1kB"></p>
<p>通过核函数得到f用作新的特征。</p>
<p>给定一个训练实例x，利用x的各个特征与预先选定的地标(landmarks)l的近似程度来选取新的特征f1,f2,f3  </p>
<p><img src="http://static.zybuluo.com/yhsdba/x7cfof2pbf5n07j2y6lcwhlk/image_1blkbmp3g10rl1t75gnu1043op580.png" alt="image_1blkbmp3g10rl1t75gnu1043op580.png-19kB">               </p>
<p>$f1=similarity(x,l^{(1)})=e(-\midx-l^{(1)}\mid^2/(2\sigma^2))$             </p>
<p>其中 $\midx-l^{(1)}\mid^2=\Sigma_{j=1}^n(x_j-l_j^{(1)})^2$  </p>
<p>similarity(x,l)就是核函数，上述是一个高斯核函数</p>
<p>注：与正态分布没什么关系，只是看起来像  </p>
<p>如果一个训练实例x与地标l的距离近似于0，则f=1,如果距离较远，f=0</p>
<p><img src="http://static.zybuluo.com/yhsdba/ckkq7f9pnij7wtddak33yh96/image_1blkc9h2qbn51t259nqnl0srp8d.png" alt="image_1blkc9h2qbn51t259nqnl0srp8d.png-403.6kB">  </p>
<p><img src="http://static.zybuluo.com/yhsdba/pljyln4y08yg8bmpjdxgaw6j/image_1blkcbcd31iqcpcvn20frlmt78q.png" alt="image_1blkcbcd31iqcpcvn20frlmt78q.png-309.3kB"></p>
<h2 id="如何选择地标"><a href="#如何选择地标" class="headerlink" title="如何选择地标"></a>如何选择地标</h2><p>通常根据训练集的数量选择地标的数量，如果训练集中有m个实例，则我们选取m个地标，并且令 $l^1=x^1,l^2=x^2,l^m=x^m$ ，这样的好处在于，新特征是建立在原有特征与训练集中所有其他特征之间的距离的基础上  </p>
<p><img src="http://static.zybuluo.com/yhsdba/tpa7po7cj5j9aqgeyxijpwtz/image_1blkcth651fjv41cbqhvq1107i97.png" alt="image_1blkcth651fjv41cbqhvq1107i97.png-363.6kB">  </p>
<p>将核函数运用的支持向量机中：</p>
<ul>
<li>给定x，计算新特征f，当 $\theta^Tx&gt;=0$ 时，预测y=1，否则y=0。修改代价函数为：</li>
</ul>
<p><img src="http://static.zybuluo.com/yhsdba/kb3bcb70n41v99qmmnqzqzot/image_1blkdfo7q7cs13dq43v1tsf1h389k.png" alt="image_1blkdfo7q7cs13dq43v1tsf1h389k.png-34.1kB">  </p>
<p>具体实施时，还需对归一化进行调整，在计算 $\Sigma_{j=1}^{n=m}\theta^2=\theta^T\theta$ 时，用 $\theta^TM\theta$ 代替 $\theta^T\theta$ ，M是根据核函数选择的一个矩阵，帮助简化计算  </p>
<p>当SVM不使用核函数时，也可称为线性核函数。当训练集特征很多而实例很少时，可以采用线性核函数向量机。  </p>
<p>下面是SVM的两个参数C和$\sigma$的影响：  </p>
<p><img src="http://static.zybuluo.com/yhsdba/l3m8weskz5zqjcsx4xd9evgs/image_1blkdnt7g1o081j2ht6t8sq122ja1.png" alt="image_1blkdnt7g1o081j2ht6t8sq122ja1.png-36kB"> </p>
<h1 id="5-使用SVM"><a href="#5-使用SVM" class="headerlink" title="5.使用SVM"></a>5.使用SVM</h1><p>其他的核函数：</p>
<ul>
<li>多项式核函数</li>
<li>字符串核函数</li>
<li>卡方核函数</li>
<li>直方图核函数</li>
<li>……</li>
</ul>
<p>使用时的一些准则：<br>n为特征数,m为训练样本数</p>
<ol>
<li>相较于m而言，n要大很多，即训练数据量不够我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机</li>
<li>如果n较小，m大小中等，如n在1-1000之间，m在10-10000之间，使用高斯核函数</li>
<li>如果n较小，m较大，如n在1-1000之间，m大于50000，在使用是svm会非常慢，解决方案是创造更多特征，然后使用逻辑回归或者线性核函数</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kingsea0-0.github.io/2017/07/24/2017-7-23-【ng公开课笔记08】SVM/" data-id="cjpcqb46f001q44v60ia43m44" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/SVM/">SVM</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/07/27/2017-7-27-【ng公开课笔记09】K-means/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          【ng公开课笔记09】聚类
        
      </div>
    </a>
  
  
    <a href="/2017/06/09/2017-5-27-【ng公开课笔记07】系统构建中的问题/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">【ng公开课笔记07】系统构建的几个问题</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nlp/">nlp</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCA/">PCA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCA-LDA/">PCA LDA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVD/">SVD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tools/">Tools</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb-数据库/">mongodb 数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据预处理/">数据预处理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习-python/">机器学习 python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概述/">概述</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫-pyhon-nlp/">爬虫 pyhon nlp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/神经网络/">神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/神经网络-word2vec/">神经网络 word2vec</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性回归/">线性回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/调参方法/">调参方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/过拟合/">过拟合</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/逻辑回归/">逻辑回归</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/PCA/" style="font-size: 10px;">PCA</a> <a href="/tags/PCA-LDA/" style="font-size: 10px;">PCA LDA</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Tools/" style="font-size: 10px;">Tools</a> <a href="/tags/mongodb-数据库/" style="font-size: 10px;">mongodb 数据库</a> <a href="/tags/word2vec/" style="font-size: 20px;">word2vec</a> <a href="/tags/数据预处理/" style="font-size: 10px;">数据预处理</a> <a href="/tags/机器学习/" style="font-size: 10px;">机器学习</a> <a href="/tags/机器学习-python/" style="font-size: 10px;">机器学习 python</a> <a href="/tags/概述/" style="font-size: 10px;">概述</a> <a href="/tags/爬虫-pyhon-nlp/" style="font-size: 10px;">爬虫 pyhon nlp</a> <a href="/tags/神经网络/" style="font-size: 10px;">神经网络</a> <a href="/tags/神经网络-word2vec/" style="font-size: 10px;">神经网络 word2vec</a> <a href="/tags/线性回归/" style="font-size: 10px;">线性回归</a> <a href="/tags/聚类/" style="font-size: 10px;">聚类</a> <a href="/tags/调参方法/" style="font-size: 10px;">调参方法</a> <a href="/tags/过拟合/" style="font-size: 10px;">过拟合</a> <a href="/tags/逻辑回归/" style="font-size: 10px;">逻辑回归</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/12/06/2018-12-01-dialogue_paper/">2018年对话系统相关论文</a>
          </li>
        
          <li>
            <a href="/2018/12/06/2018-11-28-dialogue_system_overview/">对话系统综述</a>
          </li>
        
          <li>
            <a href="/2018/12/06/2018-02-17-周志华《机器学习》笔记01 模型评估/">“周志华《机器学习》笔记01 模型评估”</a>
          </li>
        
          <li>
            <a href="/2018/12/06/2017-8-25-PCA简介/">PCA简介</a>
          </li>
        
          <li>
            <a href="/2018/12/06/2017-6-3-python的多线程和多进程/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>