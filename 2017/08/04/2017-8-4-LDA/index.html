<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  

  
  <title>LDA简介 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="LDALDA的全称是Linear Discriminant Analysis（线性判别分析），是一种supervised learning。LDA的原理是，将带标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(Linear Classifier)：因为LDA是一种线性分类器。">
<meta name="keywords" content="PCA LDA">
<meta property="og:type" content="article">
<meta property="og:title" content="LDA简介">
<meta property="og:url" content="https://kingsea0-0.github.io/2017/08/04/2017-8-4-LDA/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="LDALDA的全称是Linear Discriminant Analysis（线性判别分析），是一种supervised learning。LDA的原理是，将带标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(Linear Classifier)：因为LDA是一种线性分类器。">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/2xb0t13uos4ajiz0mji92u2c/image_1bmm1qol8t62138j1s28n1darh9.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/8hykbc5fosz03o7mouw3376v/image_1bmm1snljalr125it1t1heghe8m.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/l9jt3fyh8axc05rgmfkzue2h/image_1bmm1v7clj34b09tn11r9o199i1j.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/o91frcuvci3q1yybw3th631d/image_1bmm1vuunkht1k31d7a1eim1hlo20.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/30qj4b9cfry4phgrw19efmkt/image_1bmm20a7j618lkv1e7m6vlukn2d.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/i93zr9tjm58znrkhhpq0ojkt/image_1bmm21d15d0c1ofslo7139dldk2q.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/1lar6cb8x29mzwp6d5rkirc4/image_1bmm21pht1m7j1mb8n6daf61vir37.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/lh6waa2axf0xxq6qitn02wt9/image_1bmm2sifg106s1dogsm0hqagub3k.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/8al9xnvysnh6eg0v9kxau4so/image_1bmm2t0hb1f402duls016te146l41.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/844ujymlbw4kxbmmy39eenjs/image_1bmm2t9ubs3hj3n110huimvtm4e.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/29th473moxpndsvuaq2aiz92/image_1bmm2ukpr1b8d1vleu6df1718vt4r.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/g5w7ygqa4yx6k68krl49zcl1/image_1bmm34d9m14e712h9h71f651ups58.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/xyhmtkifwqcpxcninb4uu1rb/image_1bmm35ngg1ojf5ov19sug1vict5l.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/8ix3oqvt7cwcuhsaqm6m4336/image_1bmm8p66f1lvh1vtr11hg3nrdm57f.png">
<meta property="og:image" content="http://static.zybuluo.com/yhsdba/ah7kqp49zxsgv5d9ln8sjdvr/image_1bmm60u3911kd135lnpno661kq572.png">
<meta property="og:updated_time" content="2018-11-28T08:16:31.058Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="LDA简介">
<meta name="twitter:description" content="LDALDA的全称是Linear Discriminant Analysis（线性判别分析），是一种supervised learning。LDA的原理是，将带标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(Linear Classifier)：因为LDA是一种线性分类器。">
<meta name="twitter:image" content="http://static.zybuluo.com/yhsdba/2xb0t13uos4ajiz0mji92u2c/image_1bmm1qol8t62138j1s28n1darh9.png">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://kingsea0-0.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-2017-8-4-LDA" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/08/04/2017-8-4-LDA/" class="article-date">
  <time datetime="2017-08-04T07:00:00.000Z" itemprop="datePublished">2017-08-04</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/机器学习/">机器学习</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      LDA简介
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="LDA"><a href="#LDA" class="headerlink" title="LDA"></a>LDA</h1><p>LDA的全称是Linear Discriminant Analysis（线性判别分析），是一种supervised learning。LDA的原理是，将带标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点，会形成按类别区分，相同类别的点，将会在投影后的空间中更接近。要说明白LDA，首先得弄明白线性分类器(Linear Classifier)：因为LDA是一种线性分类器。对于K-分类的一个分类问题，会有K个线性函数：</p>
<p><img src="http://static.zybuluo.com/yhsdba/2xb0t13uos4ajiz0mji92u2c/image_1bmm1qol8t62138j1s28n1darh9.png" alt="image_1bmm1qol8t62138j1s28n1darh9.png-4kB"></p>
<p>当满足条件：对于所有的j，都有Yk &gt; Yj,的时候，我们就说x属于类别k。对于每一个分类，都有一个公式去算一个分值，在所有的公式得到的分值中，找一个最大的，就是所属的分类了。</p>
<p>上式实际上就是一种投影，是将一个高维的点投影到一条高维的直线上，LDA最求的目标是，给出一个标注了类别的数据集，投影到了一条直线之后，能够使得点尽量的按类别区分开，当k=2即二分类问题的时候，如下图所示：</p>
<p><img src="http://static.zybuluo.com/yhsdba/8hykbc5fosz03o7mouw3376v/image_1bmm1snljalr125it1t1heghe8m.png" alt="image_1bmm1snljalr125it1t1heghe8m.png-10kB"><br>下面推到一下二分类LDA问题的公式：</p>
<p>假设用来区分二分类的直线（投影函数)为：<br><img src="http://static.zybuluo.com/yhsdba/l9jt3fyh8axc05rgmfkzue2h/image_1bmm1v7clj34b09tn11r9o199i1j.png" alt="image_1bmm1v7clj34b09tn11r9o199i1j.png-1.9kB"></p>
<p>LDA分类的一个目标是使得不同类别之间的距离越远越好，同一类别之中的距离越近越好（即需要找一个最佳的w的值），所以我们需要定义几个关键的值。</p>
<p>类别i的原始中心点为：（Di表示属于类别i的点)</p>
<p> <img src="http://static.zybuluo.com/yhsdba/o91frcuvci3q1yybw3th631d/image_1bmm1vuunkht1k31d7a1eim1hlo20.png" alt="image_1bmm1vuunkht1k31d7a1eim1hlo20.png-3.5kB"></p>
<p>类别i投影后的中心点为：</p>
<p>  <img src="http://static.zybuluo.com/yhsdba/30qj4b9cfry4phgrw19efmkt/image_1bmm20a7j618lkv1e7m6vlukn2d.png" alt="image_1bmm20a7j618lkv1e7m6vlukn2d.png-3kB"></p>
<p>衡量类别i投影后，类别点之间的分散程度（方差）为：</p>
<p><img src="http://static.zybuluo.com/yhsdba/i93zr9tjm58znrkhhpq0ojkt/image_1bmm21d15d0c1ofslo7139dldk2q.png" alt="image_1bmm21d15d0c1ofslo7139dldk2q.png-5.4kB"></p>
<p>最终我们可以得到一个下面的公式，表示LDA投影到w后的损失函数：</p>
<p> <img src="http://static.zybuluo.com/yhsdba/1lar6cb8x29mzwp6d5rkirc4/image_1bmm21pht1m7j1mb8n6daf61vir37.png" alt="image_1bmm21pht1m7j1mb8n6daf61vir37.png-6.4kB"></p>
<p>  我们分类的目标是，使得类别内的点距离越近越好（集中），类别间的点越远越好。分母表示每一个类别内的方差之和，方差越大表示一个类别内的点越分散，分子为两个类别各自的中心点的距离的平方，我们最大化J(w)就可以求出最优的w了。想要求出最优的w，可以使用拉格朗日乘子法，但是现在我们得到的J(w)里面，w是不能被单独提出来的，我们就得想办法将w单独提出来。</p>
<p>我们定义一个投影前的各类别分散程度的矩阵，矩阵的含义是，如果某一个分类的输入点集Di里面的点距离这个分类的中心店mi越近，则Si里面元素的值就越小，如果分类的点都紧紧地围绕着mi，则Si里面的元素值越更接近0.</p>
<p><img src="http://static.zybuluo.com/yhsdba/lh6waa2axf0xxq6qitn02wt9/image_1bmm2sifg106s1dogsm0hqagub3k.png" alt="image_1bmm2sifg106s1dogsm0hqagub3k.png-6kB"></p>
<p>Si称作散列矩阵(scatter matrix)</p>
<p>同时定义$S_w=S_1+S_2$,Sw叫做within-class scatter matrix</p>
<p>带入Si，将J(w)分母化为：（图片中应为si的平方）</p>
<p><img src="http://static.zybuluo.com/yhsdba/8al9xnvysnh6eg0v9kxau4so/image_1bmm2t0hb1f402duls016te146l41.png" alt="image_1bmm2t0hb1f402duls016te146l41.png-19.7kB"></p>
<p><img src="http://static.zybuluo.com/yhsdba/844ujymlbw4kxbmmy39eenjs/image_1bmm2t9ubs3hj3n110huimvtm4e.png" alt="image_1bmm2t9ubs3hj3n110huimvtm4e.png-8.5kB"></p>
<p>同样的将J(w)分子化为：</p>
<p><img src="http://static.zybuluo.com/yhsdba/29th473moxpndsvuaq2aiz92/image_1bmm2ukpr1b8d1vleu6df1718vt4r.png" alt="image_1bmm2ukpr1b8d1vleu6df1718vt4r.png-10kB"></p>
<p>$S_B$成为Between-class-scatter,是一个秩为1的矩阵</p>
<p>这样损失函数可以化成下面的形式：</p>
<p><img src="http://static.zybuluo.com/yhsdba/g5w7ygqa4yx6k68krl49zcl1/image_1bmm34d9m14e712h9h71f651ups58.png" alt="image_1bmm34d9m14e712h9h71f651ups58.png-5.6kB"></p>
<p>然后就可以求导来求J(w)的最大值从而得到最佳的w。在我们求导之前，需要对分母进行归一化，因为不做归一的话，w扩大任何倍，都成立，我们就无法确定w。因此我们打算令$|w^TS_ww|=1$</p>
<p> <img src="http://static.zybuluo.com/yhsdba/xyhmtkifwqcpxcninb4uu1rb/image_1bmm35ngg1ojf5ov19sug1vict5l.png" alt="image_1bmm35ngg1ojf5ov19sug1vict5l.png-17.5kB">   </p>
<p>其中用到了矩阵微积分，求导时可以简单的把$w^TS_ww$看作$S_ww^2$</p>
<p>如果$S_w$可逆，两边同乘$S_w^{-1}$得到$S_w^{-1}S_Bw=\lambda w$</p>
<p>w就是矩阵$S_w^{-1}S_B$的特征向量</p>
<p>这个公式称为Fisher linear discrimination。</p>
<p>将前面已知的$S_B$公式带入得到$S_w^{-1}S_Bw=S_w^{-1}(u_1-u_2)*\lambda_w=\lambda w$</p>
<p>由于对w扩大缩小任何倍不影响结果，因此可以约去两边的未知常数$\lambda$和$\lambda_w$，得到</p>
<p><img src="http://static.zybuluo.com/yhsdba/8ix3oqvt7cwcuhsaqm6m4336/image_1bmm8p66f1lvh1vtr11hg3nrdm57f.png" alt="image_1bmm8p66f1lvh1vtr11hg3nrdm57f.png-1.2kB"></p>
<p>至此，我们只需要求出原始样本的均值和方差就可以求出最佳的方向w</p>
<p> 对于N(N&gt;2)分类的问题，结论：</p>
<p> <img src="http://static.zybuluo.com/yhsdba/ah7kqp49zxsgv5d9ln8sjdvr/image_1bmm60u3911kd135lnpno661kq572.png" alt="image_1bmm60u3911kd135lnpno661kq572.png-13.9kB"></p>
<p> 参考：<br> <a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html" target="_blank" rel="noopener">http://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html</a></p>
<p> <a href="http://blog.csdn.net/ffeng271/article/details/7353834" target="_blank" rel="noopener">http://blog.csdn.net/ffeng271/article/details/7353834</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://kingsea0-0.github.io/2017/08/04/2017-8-4-LDA/" data-id="cjpcqb45o000w44v6sjaw6s7m" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/PCA-LDA/">PCA LDA</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/08/15/2017-9-15-Stanford cs224d 深度学习与nlp（三）词窗与神经网络/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Stanford cs224d 深度学习与nlp(三)词窗分类与神经网络
        
      </div>
    </a>
  
  
    <a href="/2017/07/27/2017-7-23-【ng公开课笔记10】降维/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">【ng公开课笔记10】降维</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/NLP/">NLP</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/nlp/">nlp</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/工具/">工具</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/数据库/">数据库</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCA/">PCA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/PCA-LDA/">PCA LDA</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVD/">SVD</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/SVM/">SVM</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Tools/">Tools</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/mongodb-数据库/">mongodb 数据库</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/word2vec/">word2vec</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/数据预处理/">数据预处理</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习/">机器学习</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/机器学习-python/">机器学习 python</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/概述/">概述</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/爬虫-pyhon-nlp/">爬虫 pyhon nlp</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/神经网络/">神经网络</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/神经网络-word2vec/">神经网络 word2vec</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/线性回归/">线性回归</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/聚类/">聚类</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/调参方法/">调参方法</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/过拟合/">过拟合</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/逻辑回归/">逻辑回归</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/PCA/" style="font-size: 10px;">PCA</a> <a href="/tags/PCA-LDA/" style="font-size: 10px;">PCA LDA</a> <a href="/tags/SVD/" style="font-size: 10px;">SVD</a> <a href="/tags/SVM/" style="font-size: 10px;">SVM</a> <a href="/tags/Tools/" style="font-size: 10px;">Tools</a> <a href="/tags/mongodb-数据库/" style="font-size: 10px;">mongodb 数据库</a> <a href="/tags/word2vec/" style="font-size: 20px;">word2vec</a> <a href="/tags/数据预处理/" style="font-size: 10px;">数据预处理</a> <a href="/tags/机器学习/" style="font-size: 10px;">机器学习</a> <a href="/tags/机器学习-python/" style="font-size: 10px;">机器学习 python</a> <a href="/tags/概述/" style="font-size: 10px;">概述</a> <a href="/tags/爬虫-pyhon-nlp/" style="font-size: 10px;">爬虫 pyhon nlp</a> <a href="/tags/神经网络/" style="font-size: 10px;">神经网络</a> <a href="/tags/神经网络-word2vec/" style="font-size: 10px;">神经网络 word2vec</a> <a href="/tags/线性回归/" style="font-size: 10px;">线性回归</a> <a href="/tags/聚类/" style="font-size: 10px;">聚类</a> <a href="/tags/调参方法/" style="font-size: 10px;">调参方法</a> <a href="/tags/过拟合/" style="font-size: 10px;">过拟合</a> <a href="/tags/逻辑回归/" style="font-size: 10px;">逻辑回归</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/12/">December 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">August 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">July 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/06/">June 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2018/12/06/2018-12-01-dialogue_paper/">2018年对话系统相关论文</a>
          </li>
        
          <li>
            <a href="/2018/12/06/2018-11-28-dialogue_system_overview/">对话系统综述</a>
          </li>
        
          <li>
            <a href="/2018/12/06/2018-02-17-周志华《机器学习》笔记01 模型评估/">“周志华《机器学习》笔记01 模型评估”</a>
          </li>
        
          <li>
            <a href="/2018/12/06/2017-8-25-PCA简介/">PCA简介</a>
          </li>
        
          <li>
            <a href="/2018/12/06/2017-6-3-python的多线程和多进程/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2018 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>